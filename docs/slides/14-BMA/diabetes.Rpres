diabetes-pres
========================================================
author:  Merlise Clyde
date:  November 10, 2016
autosize: true

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
=========================================================


First lets load the data and coerce them to be dataframes.

```{r}
set.seed(8675309)
source("yX.diabetes.train.txt")
diabetes.train = as.data.frame(diabetes.train)

source("yX.diabetes.test.txt")
diabetes.test = as.data.frame(diabetes.test)
colnames(diabetes.test)[1] = "y"
```

=========================================================
load the library BAS 

```{r bas}
library(BAS)
diabetes.bas = bas.lm(y ~ ., data=diabetes.train, 
                      method="MCMC", 
                      n.models = 150000, 
                      thin = 10, initprobs="eplogp")

```


=================================================================
Let's check if the algorithms is even close to converging as there are $2^{64}$ models using the function  `diagnostics` 

compare estimates of posterior inclusion probabilities  estimated from Monte Carlo frequencies to estimates based on normalizing  inclusion marginal likelihoods times prior probabilities of sampled models.  These should be equal if the chain has converged.


==============================================================
```{r}
diagnostics(diabetes.bas, type="pip")

```

Close, but could run longer!  We can also compare the model probabilities 

================================================================
```{r}
diagnostics(diabetes.bas, type="model")
```

Clearly, not long enough.  One problem with MCMC and model selection is that the "best" model may be visited very few times.  

```{r}
summary(diabetes.bas$freq)
```


What variables are included?

```{r}
image(diabetes.bas)
```


Despite this, let's use BMA for prediction and compute the MSE between the predicted and actual responses for the test data using BMA:

```{r}
pred.bas = predict(diabetes.bas, newdata=diabetes.test, estimator="BMA")
cv.summary.bas(pred.bas$fit, diabetes.test$y)

```

Similary for lasso, we can compute the out od sample MSE:

```{r}
library(lars)
diabetes.lasso = lars(as.matrix(diabetes.train[, -1]), diabetes.train[,1], type="lasso")

plot(diabetes.lasso)
best = which.min(diabetes.lasso$Cp)
out.lasso = predict(diabetes.lasso, as.matrix(diabetes.test[,-1]))
cv.summary.bas(out.lasso$fit[, best], diabetes.test$y)

```


## Summary

I routinely use much larger numbers of MCMC iterations than I see in papers if looking at BMA or model selection (i.e. millions rather than say 10,000)

Explore different numbers of models - does it make a difference for estimating marginal inclusion probabilities or posterior inclusion probabilities?  What about predictions under BMA?