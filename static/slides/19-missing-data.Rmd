---
title: "STA 601: Missing data and imputation"
author: "Merlise Clyde"
date: "Nov 9, 2021"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
library(mvtnorm)
library(lattice)
library(MCMCpack)
library(hdrcde)
library(coda)
set.seed(42)
```

## Introduction to missing data

- Missing data/nonresponse is fairly common in real data. For example,
  + Failure to respond to survey question
  + Subject misses some clinic visits out of all possible
  + Only subset of subjects asked certain questions
  
--

- Recall that our posterior computation usually depends on the data through $\mathcal{p}(Y| \theta)$, which cannot be
computed (at least directly) when some of the $y_i$ values are missing.

--

- The most common software packages often throw away all subjects with incomplete data (can lead to bias and precision loss).

--

- Some individuals impute missing values with a mean or some other fixed value (ignores uncertainty).

--

- As you will see, imputing missing data is actually quite natural in the Bayesian context.



<!-- --- -->
<!-- ## Main types of nonresponse -->

<!-- - .hlight[Unit nonresponse]: the individual has no values recorded for any of the variables. For example, when participants do not complete a survey questionnaire at all. -->

<!-- - .hlight[Item nonresponse]: the individual has values recorded for at least one variable, but not all variables. -->

<!-- <table> -->
<!--   <caption>Unit nonresponse vs item nonresponse</caption> -->
<!--   <tr> -->
<!--     <th> </th> -->
<!--     <th height="30px" colspan="3">Variables</th> -->
<!--   </tr> -->
<!--   <tr> -->
<!--     <th>  </th> -->
<!--     <td height="30px" style="text-align:center" width="100px"> Y<sub>1</sub> </td> -->
<!--     <td style="text-align:center" width="100px"> Y<sub>2</sub> </td> -->
<!--     <td style="text-align:center" width="100px"> Y<sub>3</sub> </td> -->
<!--   </tr> -->
<!--   <tr> -->
<!--     <td height="30px" style="text-align:left"> Complete cases </td> -->
<!--     <td style="text-align:center"> &#10004 </td> -->
<!--     <td style="text-align:center"> &#10004 </td> -->
<!--     <td style="text-align:center"> &#10004 </td> -->
<!--   </tr> -->
<!--   <tr> -->
<!--     <td rowspan="3"> Item nonresponse </td> -->
<!--     <td rowspan="3" style="text-align:center"> &#10004 </td> -->
<!--     <td height="30px" style="text-align:center"> &#10004 </td> -->
<!--     <td style="text-align:center"> &#10067 </td> -->
<!--   </tr> -->
<!--   <tr> -->
<!--     <td height="30px" style="text-align:center"> &#10067 </td> -->
<!--     <td style="text-align:center"> &#10067 </td> -->
<!--   </tr> -->
<!--   <tr> -->
<!--     <td height="30px" style="text-align:center"> &#10067 </td> -->
<!--     <td style="text-align:center"> &#10004 </td> -->
<!--   </tr> -->
<!--   <tr> -->
<!--     <td height="30px" style="text-align:left"> Unit nonresponse </td> -->
<!--     <td style="text-align:center"> &#10067 </td> -->
<!--     <td style="text-align:center"> &#10067 </td> -->
<!--     <td style="text-align:center"> &#10067 </td> -->
<!--   </tr> -->
<!-- </table> -->



---
## Missing data mechanisms

- Data are said to be .hlight[missing completely at random (MCAR)] if the reason for missingness does not depend on the values of the observed data or missing data.

--

- For example, suppose
  - you handed out a double-sided survey questionnaire of 20 questions to a sample of participants;
  - questions 1-15 were on the first page but questions 16-20 were at the back; and
  - some of the participants did not respond to questions 16-20.
 
--
 
- Then, the values for questions 16-20 for those people who did not respond would be .hlight[MCAR] if they simply did not realize the pages were double-sided; they had no reason to ignore those questions.
 
--
 
- **This is rarely plausible in practice!**


---
## Missing data mechanisms

- Data are said to be .hlight[missing at random (MAR)] if, conditional on the values of the observed data, the reason for missingness does not depend on the missing data.

--

- Using our previous example, suppose
  - questions 1-15 include demographic information such as age and education;
  - questions 16-20 include income related questions; and
  - once again, some participants did not respond to questions 16-20.

--
  
- Then, the values for questions 16-20 for those people who did not respond would be .hlight[MAR] if younger people are more likely not to respond to those income related questions than old people, where age is observed for all participants.
  
--

- **This is the most commonly assumed mechanism in practice!**


---
## Missing data mechanisms

- Data are said to be .hlight[missing not at random (MNAR or NMAR)] if the reason for missingness depends on the actual values of the missing (unobserved) data.

--

- Continuing with our previous example, suppose again that
  - questions 1-15 include demographic information such as age and education;
  - questions 16-20 include income related questions; and
  - once again, some of the participants did not respond to questions 16-20.

--
  
- Then, the values for questions 16-20 for those people who did not respond would be .hlight[MNAR] if people who earn more money are less likely to respond to those income related questions than old people.

--
  
- **This is usually the case in real data, but analysis can be complex!**


---
## Mathematical formulation

- Consider the multivariate data scenario with $\boldsymbol{Y}_i = (\boldsymbol{Y}_1,\ldots,\boldsymbol{Y}_n)^T$, where $\boldsymbol{Y}_i = (Y_{i1},\ldots,Y_{ip})^T$, for $i = 1,\ldots, n$.

--

- For now, we will assume the multivariate normal model as the sampling model, so that each $\boldsymbol{Y}_i = (Y_{i1},\ldots,Y_{ip})^T \sim \mathcal{N}_p(\boldsymbol{\theta}, \Sigma)$.

--
	
- Suppose now that $\boldsymbol{Y}$ contains missing values.

--

- We can separate $\boldsymbol{Y}$ into the observed and missing parts, that is, $\boldsymbol{Y} = (\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis})$.

--

- Then for each individual, $\boldsymbol{Y}_i = (\boldsymbol{Y}_{i,obs},\boldsymbol{Y}_{i,mis})$.



---
## Mathematical Formulation

- Let
  + $j$ index variables (where $i$ already indexes individuals),
  + $r_{ij} = 1$ when $y_{ij}$ is missing,
  + $r_{ij} = 0$ when $y_{ij}$ is observed.

--

- Here, $r_{ij}$ is known as the missingness indicator of variable $j$ for person $i$. 

--

- Also, let 
  + $\boldsymbol{R}_i = (r_{i1},\ldots,r_{ip})^T$ be the vector of missing indicators for person $i$.
  + $\boldsymbol{R} = (\boldsymbol{R}_1,\ldots,\boldsymbol{R}_n)$ be the matrix of missing indicators for everyone.
  + $\boldsymbol{\psi}$ be the set of parameters associated with $\boldsymbol{R}$.

--

- Assume $\boldsymbol{\psi}$ and $(\boldsymbol{\theta}, \Sigma)$ are distinct.


---
## Mathematical Formulation

- MCAR:
.block[
$$p(\boldsymbol{R} | \boldsymbol{Y},\boldsymbol{\theta}, \Sigma, \boldsymbol{\psi}) = p(\boldsymbol{R} | \boldsymbol{\psi})$$
]

--

- MAR:
.block[
$$p(\boldsymbol{R} | \boldsymbol{Y},\boldsymbol{\theta}, \Sigma, \boldsymbol{\psi}) = p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{\psi})$$
]

--

- MNAR:
.block[
$$p(\boldsymbol{R} | \boldsymbol{Y},\boldsymbol{\theta}, \Sigma, \boldsymbol{\psi}) = p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis},\boldsymbol{\psi})$$
]



---
## Implications for likelihood function

- Each type of mechanism has a different implication on the likelihood of the observed data $\boldsymbol{Y}_{obs}$, and the missing data indicator $\boldsymbol{R}$.

--

- Without missingness in $\boldsymbol{Y}$, the likelihood of the observed data is
.block[
$$p(\boldsymbol{Y}_{obs} | \boldsymbol{\theta}, \Sigma)$$
]

--

- With missingness in $\boldsymbol{Y}$, the likelihood of the observed data is instead
.block[
$$
\begin{split}
p(\boldsymbol{Y}_{obs}, \boldsymbol{R} |\boldsymbol{\theta}, \Sigma, \boldsymbol{\psi}) & = \int p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis},\boldsymbol{\psi}) \cdot p(\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\
\end{split}
$$
]

 <!-- = \prod^n_{i=1}  p(\boldsymbol{Y}_{i,obs} | \boldsymbol{\theta}, \Sigma) -->

<!-- & = \int p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis},\boldsymbol{\psi}) \prod^n_{i=1}  p(\boldsymbol{Y}_{i,obs}, \boldsymbol{Y}_{i,mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\ -->

--

- Since we do not actually observe $\boldsymbol{Y}_{mis}$, we would like to be able to integrate it out so we don't have to deal with it.

--

- That is, we would like to infer $(\boldsymbol{\theta}, \Sigma)$ (and sometimes, $\boldsymbol{\psi}$) using only the observed data.



---
## Likelihood function: MCAR

- For MCAR, we have:
.block[
$$
\begin{split}
p(\boldsymbol{Y}_{obs}, \boldsymbol{R} |\boldsymbol{\theta}, \Sigma, \boldsymbol{\psi}) & = \int p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis},\boldsymbol{\psi}) \cdot p(\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\
& = \int p(\boldsymbol{R} | \boldsymbol{\psi}) \cdot p(\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\
& = p(\boldsymbol{R} | \boldsymbol{\psi}) \cdot \int p(\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\
& = p(\boldsymbol{R} | \boldsymbol{\psi}) \cdot p(\boldsymbol{Y}_{obs} | \boldsymbol{\theta}, \Sigma). \\
\end{split}
$$
]

--

- For inference on $(\boldsymbol{\theta}, \Sigma)$, we can simply focus on $p(\boldsymbol{Y}_{obs} | \boldsymbol{\theta}, \Sigma)$ in the likelihood function, since $(\boldsymbol{R} | \boldsymbol{\psi})$ does not include any $\boldsymbol{Y}$.



---
## Likelihood function: MAR

- For MAR, we have:
.block[
$$
\begin{split}
p(\boldsymbol{Y}_{obs}, \boldsymbol{R} |\boldsymbol{\theta}, \Sigma, \boldsymbol{\psi}) & = \int p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis},\boldsymbol{\psi}) \cdot p(\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\
& = \int p(\boldsymbol{R} | \boldsymbol{Y}_{obs}, \boldsymbol{\psi}) \cdot p(\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\
& = p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{\psi}) \cdot \int p(\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\
& = p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{\psi}) \cdot p(\boldsymbol{Y}_{obs} | \boldsymbol{\theta}, \Sigma). \\
\end{split}
$$
]

--

- For inference on $(\boldsymbol{\theta}, \Sigma)$, we can once again focus on $p(\boldsymbol{Y}_{obs} | \boldsymbol{\theta}, \Sigma)$ in the likelihood function

--

- Also, if we want to infer the missingness mechanism through $\boldsymbol{\psi}$, we would need to deal with $p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{\psi})$ anyway.


---
## Likelihood function: MNAR

- For MNAR, we have:
.block[
$$
\begin{split}
p(\boldsymbol{Y}_{obs}, \boldsymbol{R} |\boldsymbol{\theta}, \Sigma, \boldsymbol{\psi}) & = \int p(\boldsymbol{R} | \boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis},\boldsymbol{\psi}) \cdot p(\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis} | \boldsymbol{\theta}, \Sigma) \textrm{d}\boldsymbol{Y}_{mis} \\
\end{split}
$$
]

--

- The likelihood under MNAR cannot simplify any further.
  
--

- In this case, we cannot ignore the missing data when making inferences about $(\boldsymbol{\theta}, \Sigma)$.
  
--

- We must include the model for $\boldsymbol{R}$ and also infer the missing data $\boldsymbol{Y}_{mis}$.
  
  

---
## How to tell in practice?

- So how can we tell the type of mechanism we are dealing with? 

--

- In general, we don't know!!!

--

- Rare that data are MCAR (unless planned beforehand); more likely that data are MNAR.

--

- **Compromise**: assume data are MAR if we include enough variables in model for the missing data indicator $\boldsymbol{R}$.

--

- Whenever we talk about missing data in this course, we will do so in the context of MCAR and MAR.
  

---
## Bayesian inference with missing data

- As we have seen, for MCAR and MAR, we can focus on $p(\boldsymbol{Y}_{obs} | \boldsymbol{\theta}, \Sigma)$ in the likelihood function, when inferring $(\boldsymbol{\theta}, \Sigma)$.

--

- While this is great, for posterior sampling under most models (especially multivariate models), we actually do need all the $\boldsymbol{Y}$'s to update the parameters.

--

- In addition, we may actually want to learn about the missing values, in addition to inferring $(\boldsymbol{\theta}, \Sigma)$.

--

- By thinking of the missing data as **another set of parameters**, we can sample them from the "posterior predictive" distribution of the missing data conditional on the observed data and parameters:
.block[
.small[
$$
\begin{split}
p(\boldsymbol{Y}_{mis} | \boldsymbol{Y}_{obs},\boldsymbol{\theta}, \Sigma) \propto \prod^n_{i=1} p(\boldsymbol{Y}_{i,mis} | \boldsymbol{Y}_{i,obs},\boldsymbol{\theta}, \Sigma).
\end{split}
$$
]
]

--

- In the case of the multivariate model, each $p(\boldsymbol{Y}_{i,mis} | \boldsymbol{Y}_{i,obs},\boldsymbol{\theta}, \Sigma)$ is just a normal distribution, and we can leverage results on conditional distributions for normal models.



---
## Gibbs sampler with missing data

At iteration $s+1$, do the following

1. Sample $\boldsymbol{\theta}^{(s+1)}$ from its multivariate normal full conditional
.block[
.small[
$$p(\boldsymbol{\theta}^{(s+1)} | \boldsymbol{Y}_{obs}, \boldsymbol{Y}_{mis}^{(s)}, \Sigma^{(s)}).$$
]
]
  
--

2. Sample $\Sigma^{(s+1)}$ from its inverse-Wishart full conditional
.block[
.small[
$$p(\Sigma^{(s+1)} | \boldsymbol{Y}_{obs}, \boldsymbol{Y}_{mis}^{(s)}, \boldsymbol{\theta}^{(s+1)}).$$
]
]

--

3. For each $i = 1, \ldots, n$, with at least one "1" value in the missingness indicator vector $\boldsymbol{R}_i$, sample $\boldsymbol{Y}_{i,mis}^{(s+1)}$ from the full conditional
.block[
.small[
$$p(\boldsymbol{Y}_{i,mis}^{(s+1)} | \boldsymbol{Y}_{i,obs}, \boldsymbol{\theta}^{(s+1)}, \Sigma^{(s+1)}),$$
]
]

  which is also multivariate normal, with its form derived from the original sampling model but with the updated parameters, that is, $\boldsymbol{Y}_i^{(s+1)} = (\boldsymbol{Y}_{i,obs},\boldsymbol{Y}_{i,mis}^{(s+1)})^T \sim \mathcal{N}_p(\boldsymbol{\theta}^{(s+1)}, \Sigma^{(s+1)})$.



---
## Gibbs sampler with missing data

- Rewrite $\boldsymbol{Y}_i^{(s+1)} = (\boldsymbol{Y}_{i,mis},\boldsymbol{Y}_{i,obs}^{(s+1)})^T  \sim \mathcal{N}_p(\boldsymbol{\theta}^{(s+1)}, \Sigma^{(s+1)})$ as
.block[
.small[
\begin{eqnarray*}
\boldsymbol{Y}_i =
\begin{pmatrix}\boldsymbol{Y}_{i,mis}\\
\boldsymbol{Y}_{i,obs}
\end{pmatrix} & \sim & \mathcal{N}_p\left[\left(\begin{array}{c}
\boldsymbol{\theta}_1\\
\boldsymbol{\theta}_2
\end{array}\right),\left(\begin{array}{cc}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)\right],\\
\end{eqnarray*}
]
]

  so that we can take advantage of the conditional normal results.
  
--

- That is, we have
.block[
$$\boldsymbol{Y}_{i,mis} | \boldsymbol{Y}_{i,obs} = \boldsymbol{y}_{i,obs} \sim \mathcal{N}\left(\boldsymbol{\theta}_1 + \Sigma_{12}\Sigma_{22}^{-1}  (\boldsymbol{y}_{i,obs}-\boldsymbol{\theta}_2), \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right).$$
]

  as the multivariate normal distribution (or univariate normal distribution if $\boldsymbol{Y}_i$ only has one missing entry) we need in step 3 of the Gibbs sampler.
  
--

- This sampling technique actually encodes MAR since the imputations for $\boldsymbol{Y}_{mis}$ depend on the $\boldsymbol{Y}_{obs}$.
  
--

- Now let's look at the reading comprehension example in Hoff. We will add missing values to the original data and refit the model.
  
  
  
  
---
## Reading example with missing data

```{r}
Y <- as.matrix(dget("http://www2.stat.duke.edu/~pdh10/FCBS/Inline/Y.reading"))

#Add 20% missing data; MCAR
set.seed(1234)
Y_WithMiss <- Y #So we can keep the full data
Miss_frac <- 0.20
R <- matrix(rbinom(nrow(Y_WithMiss)*ncol(Y_WithMiss),1,Miss_frac),
            nrow(Y_WithMiss),ncol(Y_WithMiss))
Y_WithMiss[R==1]<-NA
Y_WithMiss[1:12,]
colMeans(is.na(Y_WithMiss))
```




```{r, echo=FALSE, include=FALSE}
#ACTUAL ANALYSIS STARTS HERE!!!
#Data dimensions
n <- nrow(Y_WithMiss); p <- ncol(Y_WithMiss)

#Hyperparameters for the priors
mu_0 <- c(50,50)
Lambda_0 <- matrix(c(156,78,78,156),nrow=2,ncol=2)
nu_0 <- 4
S_0 <- matrix(c(625,312.5,312.5,625),nrow=2,ncol=2)

#Define missing data indicators
##we already know R. This is to write a more general code for when we don't
R <- 1*(is.na(Y_WithMiss))
R[1:12,]
```



```{r, echo=FALSE, include=FALSE}
#Initial values for Gibbs sampler
Y_Full <- Y_WithMiss #So we can keep the data with missing values as is
for (j in 1:p) {
Y_Full[is.na(Y_Full[,j]),j] <- mean(Y_Full[,j],na.rm=TRUE) #start with mean imputation
}

Sigma <- S_0 # can't really rely on cov(Y) because we don't have full Y

#Set null objects to save samples
THETA_WithMiss <- NULL
SIGMA_WithMiss <- NULL
Y_MISS <- NULL

#first set number of iterations and burn-in, then set seed
n_iter <- 20000; burn_in <- 0.3*n_iter
```



```{r, cache=T, echo =F}
Lambda_0_inv <- solve(Lambda_0) #move outside sampler since it does not change

for (s in 1:(n_iter+burn_in)){
  ##first we must recalculate ybar inside the loop now since it changes every iteration
  ybar <- apply(Y_Full,2,mean)
  
  ##update theta
  Sigma_inv <- solve(Sigma) #invert once
  Lambda_n <- solve(Lambda_0_inv + n*Sigma_inv)
  mu_n <- Lambda_n %*% (Lambda_0_inv%*%mu_0 + n*Sigma_inv%*%ybar)
  theta <- rmvnorm(1,mu_n,Lambda_n)

  ##update Sigma
  S_theta <- (t(Y_Full)-c(theta))%*%t(t(Y_Full)-c(theta))
  S_n <- S_0 + S_theta
  nu_n <- nu_0 + n
  Sigma <- riwish(nu_n, S_n)
  
  ##update missing data using updated draws of theta and Sigma
  for(i in 1:n) {
    if(sum(R[i,]>0)){
       obs_index <- R[i,]==0
       mis_index <- R[i,]==1
       
       Sigma_22_obs_inv <- solve(Sigma[obs_index,obs_index]) #invert just once
       Sigma_12_Sigma_22_obs_inv <- Sigma[mis_index,obs_index]%*%Sigma_22_obs_inv
       
       Sigma_cond_mis <- Sigma[mis_index,mis_index] - 
         Sigma_12_Sigma_22_obs_inv%*%Sigma[obs_index,mis_index]
       
       mu_cond_mis <- theta[mis_index] + 
         Sigma_12_Sigma_22_obs_inv%*%(t(Y_Full[i,obs_index])-theta[obs_index])
      
      Y_Full[i,mis_index] <- rmvnorm(1,mu_cond_mis,Sigma_cond_mis)
      }
    }

  #save results only past burn-in
  if(s > burn_in){
  THETA_WithMiss <- rbind(THETA_WithMiss,theta)
  SIGMA_WithMiss <- rbind(SIGMA_WithMiss,c(Sigma))
  Y_MISS <- rbind(Y_MISS, Y_Full[R==1] )
  }
}

colnames(THETA_WithMiss) <- c("theta_1","theta_2")
colnames(SIGMA_WithMiss) <- c("sigma_11","sigma_12","sigma_21","sigma_22") #symmetry in sigma
```



```{r, eval=F, include=FALSE, echo=FALSE}
#library(mvtnorm) for multivariate normal
#library(MCMCpack) for inverse-Wishart

Lambda_0_inv <- solve(Lambda_0) #move outside sampler since it does not change

for (s in 1:(n_iter+burn_in)){
  ##first we must recalculate ybar inside the loop now since it changes every iteration
  ybar <- apply(Y_Full,2,mean)
  
  ##update theta
  Sigma_inv <- solve(Sigma) #invert once
  Lambda_n <- solve(Lambda_0_inv + n*Sigma_inv)
  mu_n <- Lambda_n %*% (Lambda_0_inv%*%mu_0 + n*Sigma_inv%*%ybar)
  theta <- rmvnorm(1,mu_n,Lambda_n)

  ##update Sigma
  S_theta <- (t(Y_Full)-c(theta))%*%t(t(Y_Full)-c(theta))
  S_n <- S_0 + S_theta
  nu_n <- nu_0 + n
  Sigma <- riwish(nu_n, S_n)
```




```{r, eval=F, include=FALSE}
##update missing data using updated draws of theta and Sigma
  for(i in 1:n) {
    if(sum(R[i,]>0)){
       obs_index <- R[i,]==0
       mis_index <- R[i,]==1
       Sigma_22_obs_inv <- solve(Sigma[obs_index,obs_index]) #invert just once
       Sigma_12_Sigma_22_obs_inv <- Sigma[mis_index,obs_index]%*%Sigma_22_obs_inv
       
       Sigma_cond_mis <- Sigma[mis_index,mis_index] - 
         Sigma_12_Sigma_22_obs_inv%*%Sigma[obs_index,mis_index]
       
       mu_cond_mis <- theta[mis_index] + 
         Sigma_12_Sigma_22_obs_inv%*%(t(Y_Full[i,obs_index])-theta[obs_index])
      
      Y_Full[i,mis_index] <- rmvnorm(1,mu_cond_mis,Sigma_cond_mis)
      }
    }

  #save results only past burn-in
  if(s > burn_in){
  THETA_WithMiss <- rbind(THETA_WithMiss,theta)
  SIGMA_WithMiss <- rbind(SIGMA_WithMiss,c(Sigma))
  Y_MISS <- rbind(Y_MISS, Y_Full[R==1] )
  }
}

colnames(THETA_WithMiss) <- c("theta_1","theta_2")
colnames(SIGMA_WithMiss) <- c("sigma_11","sigma_12","sigma_21","sigma_22") #symmetry in sigma
```





```{r fig.height=4, include=FALSE, echo=FALSE}
#library(coda)
THETA_WithMiss.mcmc <- mcmc(THETA_WithMiss,start=1); summary(THETA_WithMiss.mcmc)
```




```{r fig.height=4, include-FALSE}
SIGMA_WithMiss.mcmc <- mcmc(SIGMA_WithMiss,start=1); summary(SIGMA_WithMiss.mcmc)
```


---
## Compare to inference from full data

With missing data:
```{r fig.height=4}
apply(THETA_WithMiss,2,summary)
```

Based on true data:
```{r, echo=F, cache=T}
Y <- as.matrix(dget("http://www2.stat.duke.edu/~pdh10/FCBS/Inline/Y.reading"))
#Data summaries
n <- nrow(Y)
ybar <- apply(Y,2,mean)

#Hyperparameters for the priors
mu_0 <- c(50,50)
Lambda_0 <- matrix(c(156,78,78,156),nrow=2,ncol=2)
nu_0 <- 4
S_0 <- matrix(c(625,312.5,312.5,625),nrow=2,ncol=2)

#Initial values for Gibbs sampler
#No need to set initial value for theta, we can simply sample it first
Sigma <- cov(Y)

#Set null matrices to save samples
THETA <- SIGMA <- NULL

#Now, to the Gibbs sampler
#library(mvtnorm) for multivariate normal
#library(MCMCpack) for inverse-Wishart

#first set number of iterations and burn-in, then set seed
n_iter <- 20000; burn_in <- 0.3*n_iter
set.seed(1234)

for (s in 1:(n_iter+burn_in)){
##update theta using its full conditional
Lambda_n <- solve(solve(Lambda_0) + n*solve(Sigma))
mu_n <- Lambda_n %*% (solve(Lambda_0)%*%mu_0 + n*solve(Sigma)%*%ybar)
theta <- rmvnorm(1,mu_n,Lambda_n)

#update Sigma
S_theta <- (t(Y)-c(theta))%*%t(t(Y)-c(theta))
S_n <- S_0 + S_theta
nu_n <- nu_0 + n
Sigma <- riwish(nu_n, S_n)

#save results only past burn-in
if(s > burn_in){
  THETA <- rbind(THETA,theta)
  SIGMA <- rbind(SIGMA,c(Sigma))
  }
}
colnames(THETA) <- c("theta_1","theta_2")
colnames(SIGMA) <- c("sigma_11","sigma_12","sigma_21","sigma_22") #symmetry in sigma

THETA.mcmc <- mcmc(THETA,start=1)
SIGMA.mcmc <- mcmc(SIGMA,start=1)
```

```{r fig.height=4}
apply(THETA,2,summary) 
```

Very similar for the most part.

---
## Compare to inference from full data

With missing data:
```{r fig.height=4}
apply(SIGMA_WithMiss,2,summary)
```

Based on true data:
```{r fig.height=4}
apply(SIGMA,2,summary) 
```

Also very similar. A bit more uncertainty in dimension of $Y_{i2}$ because we have more missing data there.



---
## Posterior distribution of the mean

```{r fig.height=4.8, echo=F}
theta.kde <- kde2d(THETA_WithMiss[,1], THETA_WithMiss[,2], n = 50)
image(theta.kde,xlab=expression(theta[1]),ylab=expression(theta[2]))
contour(theta.kde, add = T)
```



---
## Missing data vs predictions for new observations

- How about predictions for completely new observations?

--

- That is, suppose your original dataset plus sampling model is $\boldsymbol{y_i} = (y_{i,1},y_{i,2})^T \sim \mathcal{N}_2(\boldsymbol{\theta}, \Sigma)$, $i = 1, \ldots, n$.

--

- Suppose now you have $n^\star$ new observations with $y_{2}^\star$ values but no $y_{1}^\star$.

--

- How can we predict $y_{i,1}^\star$ given $y_{i,2}^\star$, for $i = 1, \ldots, n^\star$?

--

- Well, we can view this as a "train $\rightarrow$ test" prediction problem rather than a missing data problem on an original data.


---
## Missing data vs predictions for new observations

- That is, given the posterior samples of the parameters, and the test values for $y_{i2}^\star$, draw from the posterior predictive distribution of $(y_{i,1}^\star | y_{i,2}^\star, \{(y_{1,1},y_{1,2}), \ldots, (y_{n,1},y_{n,2})\})$. 

--

- To sample from this predictive distribution, think of compositional sampling.

--

- That is, for each posterior sample of $(\boldsymbol{\theta}, \Sigma)$, sample from $(y_{i,1} | y_{i,2}, \boldsymbol{\theta}, \Sigma)$, which is just from the form of the sampling distribution.

--

- In this case, $(y_{i,1} | y_{i,2}, \boldsymbol{\theta}, \Sigma)$ is just a normal distribution derived from $(y_{i,1}, y_{i,2} | \boldsymbol{\theta}, \Sigma)$, based on the conditional normal formula.

--

- No need to incorporate the prediction problem into your original Gibbs sampler!


