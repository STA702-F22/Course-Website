<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Bayesian Model Choice in Linear Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Merlise Clyde" />
    <meta name="date" content="2022-10-27" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Bayesian Model Choice in Linear Regression
]
.subtitle[
## STA 702 Fall 2022
]
.author[
### Merlise Clyde
]
.date[
### October 27, 2022
]

---


`$$\begin{equation*}
\newcommand{\Y}{\boldsymbol{Y}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\b}{\boldsymbol{\beta}}
\newcommand{\bhat}{\hat{\boldsymbol{\beta}}}
\newcommand{\SSE}{\textsf{SSE}}
\newcommand{\SS}{\textsf{SS}}
\newcommand{\bv}{\mathbf{b}}
\newcommand{\N}{\textsf{N}}
\newcommand{\Ber}{\textsf{Ber}}
\newcommand{\Poi}{\text{Poi}}
\newcommand{\Gam}{\textsf{Gamma}}
\newcommand{\Gm}{\textsf{G}}
\end{equation*}$$`





--
## Bayesian Model Choice

**General setting:**

1. Define a list of models; let `\(\Gamma\)` be a "finite" set of different possible models. 
  
--

2. Each model `\(\gamma\)` is in `\(\Gamma\)`, including the "true" model. Also, let `\(\theta_\gamma\)` represent the parameters in model `\(\gamma\)`.
  
--

3. Put a prior over the set `\(\Gamma\)`. Let `\(\Pi_\gamma = p[\gamma] = \Pr[\gamma \text{ is true}]\)`, for all `\(\gamma \in \Gamma\)`.
  
--

4. Put a prior on the parameters in each model, that is, each `\(\pi(\theta_\gamma)\)`.

--

5. Compute marginal posterior probabilities `\(\Pr[\gamma | Y]\)` for each model, and select a model based on the posterior probabilities or use the full posterior over all models!

---
## Bayesian Model Probabilities

- For each model `\(\gamma \in \Gamma\)`, we need to compute `\(\Pr[\gamma | Y]\)`. 

--

- Let `\(p_\gamma(Y)\)` denote the marginal likelihood of the data under model `\(\gamma\)`, that is, `\(\mathcal{p}[Y | \gamma]\)`. As before,
.block[
.small[
$$
`\begin{split}
\hat{\Pi}_\gamma = \Pr[\gamma | Y] &amp; = \frac{\mathcal{p}[Y | \gamma] \cdot p[\gamma]}{\sum_{\gamma^\star \in \Gamma} \mathcal{p}[Y | \gamma^\star] \cdot p[\gamma^\star]} = \frac{ p_\gamma(Y) \Pi_\gamma }{ \sum_{\gamma^\star \in \Gamma} p_{\gamma^\star}(Y) \Pi_{\gamma^\star} }\\
\\
&amp; = \frac{ \Pi_\gamma \cdot \left[ \int_{\Theta_\gamma} p_\gamma(Y | \theta_\gamma) \cdot \pi(\theta_\gamma) \text{d}\theta_\gamma \right] }{ \sum_{\gamma^\star \in \Gamma} \Pi_{\gamma^\star} \cdot \left[ \int_{\Theta_{\gamma^\star}} p_{\gamma^\star}(Y | \theta_{\gamma^\star}) \cdot \pi(\theta_{\gamma^\star}) \text{d}\theta_{\gamma^\star} \right] }.\\
\end{split}`
$$
]
]

--

- If we assume a uniform prior on `\(\Gamma\)`, that is, `\(\Pi_\gamma = \frac{1}{\#\Gamma}\)`, for all `\(\gamma \in \Gamma\)`, then
.block[
.small[
`$$\hat{\Pi}_\gamma  = \frac{ p_\gamma(Y) }{ \sum_{\gamma^\star \in \Gamma} p_{\gamma^\star}(Y)  }
= \frac{ \left[ \int_{\Theta_\gamma} p_\gamma(Y | \theta_\gamma) \cdot \pi(\theta_\gamma) \text{d}\theta_\gamma \right] }{ \sum_{\gamma^\star \in \Gamma} \left[ \int_{\Theta_{\gamma^\star}} p_{\gamma^\star}(Y | \theta_{\gamma^\star}) \cdot \pi(\theta_{\gamma^\star}) \text{d}\theta_{\gamma^\star} \right]  }$$`
]]


---
## Bayesian Model Selection 

- &lt;div class="question"&gt;
How should we choose the Bayes optimal model?
&lt;/div&gt;

--

- We can specify a loss function. The most common is
.block[
.small[
$$
`\begin{split}
L(\hat{\gamma},\gamma) = \boldsymbol{1(\hat{\gamma} \neq \gamma)},
\end{split}`
$$
]
]

  that is,
  1. Loss equals zero if the correct model is chosen; and
  2. Loss equals one if incorrect model is chosen.
  
--

- Next, select `\(\hat{\gamma}\)` to minimize Bayes risk. Here, Bayes risk (expected loss over posterior) is
.block[
.small[
$$
`\begin{split}
R(\hat{\gamma}) = \sum_{\gamma \in \Gamma} \boldsymbol{1(\hat{\gamma} \neq \gamma)} \cdot \hat{\Pi}_\gamma = 0 \cdot \hat{\Pi}_{\gamma_{\text{true}}} +   \sum_{\gamma \neq \gamma_{\text{true}}} \hat{\Pi}_\gamma = \sum_{\gamma \neq \hat{\gamma}} \hat{\Pi}_\gamma = 1 - \hat{\Pi}_{\hat{\gamma}}
\end{split}`
$$
]
]

--

- To minimize `\(R(\hat{\gamma})\)`, choose `\(\hat{\gamma}\)` such that `\(\hat{\Pi}_{\hat{\gamma}}\)` is the largest! That is, select the model with the largest posterior probability.



---
## Inference vs prediction

- What if the goal is prediction? Then maybe we should care more about predictive accuracy, rather than selecting specific variables.

--

- For predictions, we care about the posterior predictive distribution, that is
.block[
.small[
$$
`\begin{split}
p(y_{n+1}|Y = (y_1, \ldots, y_n)) &amp; = \int_\Gamma \int_{\Theta_\gamma} p(y_{n+1}|\gamma, \theta_\gamma) \cdot \pi(\gamma, \theta_\gamma | Y)  \ \text{d}\theta_\gamma \text{d}\gamma  \\
&amp; = \int_\Gamma \int_{\Theta_\gamma} p(y_{n+1}|\gamma, \theta_\gamma) \cdot \pi(\theta_\gamma | Y, \gamma) \cdot \Pr[\gamma | Y]  \ \text{d}\theta_\gamma \text{d}\gamma  \\
&amp; = \sum_{\gamma \in \Gamma} \int_{\Theta_\gamma} p(y_{n+1}|\gamma, \theta_\gamma) \cdot \pi(\theta_\gamma | Y, \gamma) \cdot \hat{\Pi}_\gamma  \ \text{d}\theta_\gamma \\
&amp; = \sum_{\gamma \in \Gamma} \hat{\Pi}_\gamma \cdot \int_{\Theta_\gamma} p(y_{n+1}|\gamma, \theta_\gamma) \cdot \pi(\theta_\gamma | Y, \gamma)  \ \text{d}\theta_\gamma \\
&amp; = \sum_{\gamma \in \Gamma} \hat{\Pi}_\gamma \cdot p(y_{n+1} | Y, \gamma), \\
\end{split}`
$$
]
]

  which is just averaging out the predictions from each model, over all possible models in `\(\Gamma\)`, with the posterior probability of each model, and this is known as .hlight[Bayesian model averaging (BMA)].



---
## Bayesian Linear Regression 


** Practical Issues:** the posterior probability that the model is true
.block[
.small[
$$
`\begin{split}
\hat{\Pi}_\gamma &amp; = \frac{ \Pi_\gamma p_\gamma(Y) }{ \sum_{\gamma^\star \in \Gamma} \Pi_{\gamma^\star} p_{\gamma^\star}(Y)  }.\\
\end{split}`
$$
]
]

--


- We need to calculate marginal likelihoods for ALL models in `\(\Gamma\)` 
  
--


- In general for, we cannot calculate the marginal likelihoods unless we have a proper or conjugate priors  (Normal-Gamma priors within each model)  
  
--

- We need to specify proprer prior distributions on  all common parameters `\(\theta_\gamma\)` in each models!  Conventional priors such as Zellner's g-prior or Ridge Regression to reduce elicitation of prior covariances

--

- Can put priors on hyperparameters cases and integrate or use numerical approximations! 

--

- May not be able to enumerate! Gibbs or MCMC for more flexibility!



---
## Bayesian Variable Selection  (BVS)

- Rewrite each model `\(\gamma \in \Gamma\)` as
.block[
`$$\boldsymbol{Y} \mid \alpha, \boldsymbol{\beta}_{\gamma}, \gamma, \phi \sim \mathcal{N}_n(\mathbf{1}_n \alpha + \boldsymbol{X}_{\gamma}\boldsymbol{\beta}_{\gamma}, \phi^{-1}\boldsymbol{I}_{n\times n})$$`
]

--

- `\(\gamma\)` represents the set of predictors we want to include in  our model.

--

-  `\(\gamma = (\gamma_1, \ldots, \gamma_{p}) \in \{0,1\}^p\)`, so that the cardinality of `\(\Gamma\)` is `\(2^p\)`, the number of models in `\(\Gamma\)`.

--

`$$\gamma_j =  \left\{ \begin{array}{cl} 1 &amp; \text{ if the j'th predictor is included in the model} \\ 0 &amp; \text{ if it is not} \end{array}  \right.$$`
 
- `\(p_\gamma \equiv \sum^p_{j=1} \gamma_j\)`, so that `\(p_\gamma\)` is the number of predictors included in model `\(\gamma\)`
  
- `\(\boldsymbol{X}_{\gamma}\)` ( `\(n \times p_\gamma\)` ) is the matrix of predictors with `\(\gamma_j = 1\)` (wolg design matrix with centered columns)

- `\(\boldsymbol{\beta}_{\gamma}\)` ( `\(p_\gamma \times 1\)` )is the corresponding vector of predictors with `\(\gamma_j = 1\)` 
  


---
## BVS

- Recall that we can also write each model as
.block[
.small[
`$$Y_i =  1 \alpha + \boldsymbol{\beta}^T_{\gamma} \boldsymbol{x}_{i\gamma} + \epsilon_i; \ \ \ \  \epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \phi^{-1}).$$`
]
]
  
--

- As an example, suppose we had data with 5 potential predictors including the intercept, so that each `\(\boldsymbol{x}_i = (x_{i1}, x_{i2}, x_{i3},x_{i4},x_{i5})\)`, and `\(\boldsymbol{\beta} = (\beta_1, \beta_2, \beta_3, \beta_4, \beta_5)\)`.

--

- Then for model with `\(\gamma = (1,0,0,0,0)\)`, `\(Y_i = \boldsymbol{\beta}^T_{\gamma} \boldsymbol{x}_{i\gamma} + \epsilon_i\)`
.block[
.small[
`$$\implies Y_i = \alpha + \beta_1 x_{i1} + \epsilon_i; \ \ \ \  \epsilon_i \overset{iid}{\sim} \mathcal{N}(0, 1/\phi),$$`
]
]

  with `\(p_\gamma = 1\)`.

- Whereas for model with `\(\gamma = (0,0,1,1,0)\)`, `\(Y_i = \alpha +  \boldsymbol{\beta}^T_{\gamma} \boldsymbol{x}_{i\gamma} + \epsilon_i\)`
.block[
.small[
`$$\implies Y_i = \alpha + \beta_3 x_{i3} + \beta_4 x_{i4} + \epsilon_i; \ \ \ \  \epsilon_i \overset{iid}{\sim} \mathcal{N}(0, \sigma^21/\phi),$$`
]
]

  with `\(p_\gamma = 2\)`.



---
## Steps

The outline for variable selection would be as follows:

1) Write down likelihood under model `\(\gamma\)`. That is,
.block[
.small[
$$
`\begin{split}
p(\boldsymbol{y} | \boldsymbol{X}, \gamma, \alpha, \boldsymbol{\beta}_{\gamma}, \phi) &amp; \propto (\sigma^2)^{-\frac{n}{2}} \ \textrm{exp} \left\{-\frac{\phi}{2} (\boldsymbol{y} - \mathbf{1} \alpha - \boldsymbol{X}_{\gamma}\boldsymbol{\beta}_{\gamma})^T (\boldsymbol{y} - \mathbf{1} \alpha -\boldsymbol{X}_{\gamma}\boldsymbol{\beta}_{\gamma})\right\}\\
\end{split}`
$$
]
]

--

2) Define a prior for `\(\gamma\)`, `\(\Pi_\gamma = \Pr[\gamma]\)`. 

  + `\(p(\gamma_j = 1) = .5 \Rightarrow p(\gamma) = .5^p\)`  Uniform on space of models and `\(p_\gamma \sim \textsf{Bin}(p, .5)\)`
  
  + `\(\gamma_j \mid \pi \overset{iid}{\sim} \textsf{Ber}(\pi)\)` and `\(\pi \sim \textsf{Beta}(a,b)\)` then  `\(p_\gamma \sim \textsf{Beta-Binomial}(a, b)\)`
  
`$$p(p_\gamma \mid p, a, b) = \frac{\Gamma(p + 1) \Gamma(p_\gamma + a) \Gamma(p - p_\gamma + b) \Gamma (a + b) }{\Gamma(p_\gamma+1) \Gamma(p - p_\gamma + 1) \Gamma(p + a + b) \Gamma(a) \Gamma(b)}$$`
`$$p_\gamma \sim \textsf{Beta-Binomial}(1, 1) \sim \textsf{Unif}(0, p)$$`
---
## Prior on model specific parameters

3) Using independent Jeffrey's priors on common parameters and the g-prior  we have

`$$\begin{align*}
\pi(\alpha, \phi) &amp; = \phi^{-1}\\
\pi(\boldsymbol{\beta}_{\gamma} | \phi) &amp; = \textsf{N}_p\left(\boldsymbol{\beta}_{0\gamma}= \boldsymbol{0}, \Sigma_{0\gamma} = \frac{g}{\phi} \left[{\boldsymbol{X}_{\gamma}}^T \boldsymbol{X}_{\gamma} \right]^{-1} \right) \\ 
\end{align*}$$`




---
## Posteriors

- With those pieces, the conditional posteriors are straightforward 

`$$\begin{align*}\alpha \mid \phi, y &amp; \sim \textsf{N}\left(\bar{y}, \frac{1}{n \phi}\right)\\
\boldsymbol{\beta}_{\gamma} \mid \gamma, \phi, g, y &amp;\sim \textsf{N}\left( \frac{g}{1 + g} \hat{\boldsymbol{\beta}}_{\gamma}, \frac{g}{1 + g} \frac{1}{\phi} \left[{\boldsymbol{X}_{\gamma}}^T \boldsymbol{X}_{\gamma} \right]^{-1}  \right) \\
\phi \mid \gamma, y &amp; \sim \textsf{Gamma}( \cdot , \cdot ) \\
p(\gamma \mid y) &amp; \propto p(y \mid \gamma) p(\gamma) 
\end{align*}$$`

--
 
- due to conjugacy, the marginal likelihood of `\(\gamma\)` is proportional
to 

`$$p(Y \mid \gamma) = C (1 + g)^{\frac{n-p_\gamma -1}{2}} ( 1 + g (1 - R^2_\gamma))^{- \frac{(n-1)}{2}}$$`
 
- `\(R^2_\gamma\)` is the usual coefficient of determination for model `\(\gamma\)`, 

`$$R^2_\gamma = 1 - \frac{(y - \hat{y}_{\gamma})^T(y - \hat{y}_{\gamma})}{(y -  \mathbf{1}\bar{y})^T (y -  \mathbf{1} \bar{y})}$$`


--

-  we can run a collapsed Gibbs or MH sampler over just `\(\Gamma\)`! 

---
## Summaries

- We can then compute marginal posterior probabilities `\(\Pr[\gamma | Y]\)` for each model and select model with the highest posterior probability.

--

- We can also compute posterior `\(\Pr[\gamma_j = 1 \mid Y]\)`, the posterior probability of including the `\(j\)`'th predictor, often called .hlight[marginal inclusion probability (MIP)], allowing for uncertainty in the other predictors.

--

- Also straightforward to do model averaging once we all have posterior samples.

--

- The Hoff book works through one example and you can find the Gibbs sampler for doing inference there. I strongly recommend you go through it carefully!

--

- Also paper by Liang et al (2008)  JASA


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
