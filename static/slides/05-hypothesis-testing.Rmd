---
title: "Lecture 5: Basics of Bayesian Hypothesis Testing"
author: "Merlise Clyde"
date: "September 9, 2021"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rvest)
```


## Hypothesis Testing


Suppose we have univariate data $y_i \overset{iid}{\sim} \mathcal{N}(\theta, 1)$ 

--

goal is to test $\mathcal{H}_0: \theta = 0; \ \ \text{vs } \mathcal{H}_1: \theta \neq 0$ 

--

Frequentist testing - likelihood ratio, Wald, score, UMP,  confidence regions, etc

--

- Need a **test statistic** $T(y^{(n)})$  (and its sampling distribution)



```{r teststatdist, fig=TRUE, echo=FALSE, fig.height=5, fig.width=5, out.width="25%"}

t=seq(-4,4, length=1000)
plot(t, dt(t, 3), type="l", lty=1, lwd=2,
     ylab="density",
     xlab=expression(T(y^(n))))
```

--

- **p-value**: Calculate the probability of seeing a dataset/test statistics as extreme or more extreme than the oberved data with repeated sampling under the null hypothesis

---
## Errors

if p-value is less than a pre-specified $\alpha$ then reject $\mathcal{H}_0$ in favor of $\mathcal{H}_1$ 

--

- Type I error:  falsely concluding  in favor of $\mathcal{H}_1$  when $\mathcal{H}_0$ is true


--

- To maintain a Type I error rate of  $\alpha$, then we reject $\mathcal{H}_0$ in favor of $\mathcal{H}_1$  when $p < \alpha$

--

For this to be a valid frequents test the p-value must have a uniform distribution under $\mathcal{H}_0$


--

- Type II error: failing to conclude in favor of  $\mathcal{H}_1$ when  $\mathcal{H}_1$ is true

--

- 1 - P(Type II error) is the **power** of the test


--

**Note:** we _never_ conclude in favor of $\mathcal{H}_0$.  We are looking for enough evidence to reject $\mathcal{H}_0$.  But if we fail to reject we do not conclude that it is true!

---
## Bayesian Approach



  1. Put a prior on $\theta$, $\pi(\theta) = \mathcal{N}(\theta_0, 1/\tau_0^2)$.

--

  2. Compute posterior $\theta  \mid y^{(n)} \sim \mathcal{N}(\theta_n, 1/\tau_n^2)$ for updated parameters $\theta_n$ and $\tau_n^2$.
  
  
--

```{r postdist, fig=TRUE, echo=FALSE, fig.height=5, fig.width=5, out.width="50%"}

t=seq(-4,4, length=1000)
plot(t+2.9, dt(t, 3), type="l", lty=1, lwd=2,
     xlab=expression(theta), ylab="posterior density")
abline(v=0)
```

---
## Informal 

**Credible Intervals**

  1. Compute a 95% CI based on the posterior.

--

  2. Reject $\mathcal{H}_0$ if interval does not contain zero.
  

--

**Tail Areas**:


1.  Compute $\Pr(\theta > 0 \mid y^{(n)})$ and $\Pr(\theta < 0 \mid y^{(n)})$ 

--

2. Report minimum of these probabilities  as a "Bayesian p-value"


Note:  Tail probability is not the same as $\Pr(\mathcal{H}_0 \mid y^{(n)})$

---
## Formal Bayesian Hypothesis Testing

Unknowns are $\mathcal{H}_0$ and $\mathcal{H}_1$

Put a prior on the actual hypotheses/models, that is, on $\pi(\mathcal{H}_0) = \Pr(\mathcal{H}_0 = \text{True})$ and $\pi(\mathcal{H}_1) = \Pr(\mathcal{H}_1 = \text{True})$.
      
--
-      For example, set $\pi(\mathcal{H}_0) = 0.5$ and $\pi(\mathcal{H}_1) = 0.5$, if _a priori_, we believe the two hypotheses are equally likely.

--
Likelihood of the hypotheses 
   
      $$\cal{L}(\mathcal{H}_i) \propto p(y^{(n)} \mid \mathcal{H}_i)$$
      
--


$$p(y^{(n)} \mid \mathcal{H}_0) = \prod_{i = 1}^n (2 \pi)^{-1/2} \exp{- \frac{1}{2} (y_i - 0)^2}$$
--

$$p(y^{(n)} \mid \mathcal{H}_1)  = \int_\Theta p(y^{(n)} \mid \mathcal{H}_1, \theta) p(\theta \mid \mathcal{H}_1) \, d\theta$$
---
## Bayesian Approach 

Priors on parameters under each hypothesis

--

In our simple normal model, the only unknown parameter is $\theta$

--

- under $\mathcal{H}_0$, $\theta = 0$ with probability 1

--

- under $\mathcal{H}_0$, $\theta \in \mathbb{R}$
Could take $\pi(\theta) = \mathcal{N}(\theta_0, 1/\tau_0^2)$.

--

-   Compute marginal likelihoods for each hypothesis, that is, $\cal{L}(\mathcal{H}_0)$ and $\cal{L}(\mathcal{H}_1)$.  

--

- Obtain posterior probabilities of $\cal{H}_0$ and $\cal{H}_1$ via Bayes Theorem.


---
## Bayesian Approach - Decisions

Loss function for hypothesis testing

 - $\hat{\cal{H}}$ is the chosen hypothesis
 
 - $\cal{H}_{\text{true}}$ is the true hypothesis, $\cal{H}$ for short
 
--

Two types of errors:

- Type I error:  $\hat{\cal{H}} = 1$  and  $\cal{H} = 0$

--

- Type II error:  $\hat{\cal{H}} = 0$  and  $\cal{H} = 1$

--

Loss function:
$$L(\hat{\cal{H}}, \cal{H}) =  w_1  \, 1(\hat{\cal{H}} = 1, \cal{H} = 0) + w_2 \, 1(\hat{\cal{H}} = 0, \cal{H} = 1)$$
- $w_1$ weights how bad  making a Type I error

- $w_2$ weights how bad making a Type II error

---
## Loss Function Functions and Decisions

- Relative weights
$$L(\hat{\cal{H}}, \cal{H}) =   \, 1(\hat{\cal{H}} = 1, \cal{H} = 0) + w \, 1(\hat{\cal{H}} = 0, \cal{H} = 1)$$
--

- Special case $w=1$

$$L(\hat{\cal{H}}, \cal{H}) =    1(\hat{\cal{H}} \neq \cal{H})$$ 
- known as 0-1 loss (most common)

--

- Bayes Risk (Posterior Expected Loss)

$$\textsf{E}_{\cal{H} \mid y^{(n)}}[L(\hat{\cal{H}}, \cal{H}) ] =
1(\hat{\cal{H}} = 1)\pi(\cal{H}_0 \mid y^{(n)}) +  1(\hat{\cal{H}} = 0) \pi(\cal{H}_1 \mid y^{(n)})$$



--

- Minimize loss by picking hypothesis with the highest posterior probability 



---
## Bayesian hypothesis testing

- Using Bayes theorem,
.block[
.small[
$$
\begin{split}
\pi(\mathcal{H}_1 \mid Y) = \frac{ p(y^{(n)} \mid \mathcal{H}_1) \pi(\mathcal{H}_1) }{ p(y^{(n)} \mid \mathcal{H}_0) \pi(\mathcal{H}_0) + p(y^{(n)} \mid \mathcal{H}_1) \pi(\mathcal{H}_1)},
\end{split}
$$
]
]

  where $p(y^{(n)} \mid \mathcal{H}_0)$ and $p(y^{(n)} \mid \mathcal{H}_1)$ are the marginal likelihoods hypotheses.
  
--

- If for example we set $\pi(\mathcal{H}_0) = 0.5$ and $\pi(\mathcal{H}_1) = 0.5$ _a priori_, then
.block[
.small[
$$
\begin{split}
\pi(\mathcal{H}_1 \mid Y) & = \frac{ 0.5 p(y^{(n)} \mid \mathcal{H}_1) }{ 0.5 p(y^{(n)} \mid \mathcal{H}_0) + 0.5 p(y^{(n)} \mid \mathcal{H}_1) } \\
\\
& = \frac{ p(y^{(n)} \mid \mathcal{H}_1) }{ p(y^{(n)} \mid \mathcal{H}_0) + p(y^{(n)} \mid \mathcal{H}_1) }= \frac{ 1 }{ \frac{p(y^{(n)} \mid \mathcal{H}_0)}{p(y^{(n)} \mid \mathcal{H}_1)} + 1 }.\\
\end{split}
$$
]
]

--

- The ratio $\frac{p(y^{(n)} \mid \mathcal{H}_0)}{p(y^{(n)} \mid \mathcal{H}_1)}$ is known as the **Bayes factor** in favor of $\mathcal{H}_0$, and often written as $\mathcal{BF}_{01}$. Similarly, we can compute $\mathcal{BF}_{10}$.



---
## Bayes factors

- **Bayes factor**: is a ratio of marginal likelihoods and it provides a weight of evidence in the data in favor of one model over another.

--

- It is often used as an alternative to the frequentist p-value.

--

- **Rule of thumb**: $\mathcal{BF}_{01} > 10$ is strong evidence for $\mathcal{H}_0$;  $\mathcal{BF}_{01} > 100$ is decisive evidence for $\mathcal{H}_0$.

--

- Notice that for our example,
.block[
.small[
$$
\begin{split}
\pi(\mathcal{H}_1 \mid Y) = \frac{ 1 }{ \frac{p(y^{(n)} \mid \mathcal{H}_0)}{p(y^{(n)} \mid \mathcal{H}_1)} + 1 } = \frac{ 1 }{ \mathcal{BF}_{01} + 1 } \\
\end{split}
$$
]
]

  the higher the value of $\mathcal{BF}_{01}$, that is, the weight of evidence in the data in favor of $\mathcal{H}_0$, the lower the marginal posterior probability that $\mathcal{H}_1$ is true.
  
--

- That is, here, as $\mathcal{BF}_{01} \uparrow$, $\pi(\mathcal{H}_1 \mid Y) \downarrow$.




---
## Bayes factors

- Let's look at another way to think of Bayes factors. First, recall that
.block[
.small[
$$
\begin{split}
\pi(\mathcal{H}_1 \mid Y) = \frac{ p(y^{(n)} \mid \mathcal{H}_1) \pi(\mathcal{H}_1) }{ p(y^{(n)} \mid \mathcal{H}_0) \pi(\mathcal{H}_0) + p(y^{(n)} \mid \mathcal{H}_1) \pi(\mathcal{H}_1)},
\end{split}
$$
]
]

  so that
.block[
.small[
$$
\begin{split}
\frac{\pi(\mathcal{H}_0 | Y)}{\pi(\mathcal{H}_1 | Y)} & = \frac{ p(y^{(n)} |\mathcal{H}_0) \pi(\mathcal{H}_0) }{ p(y^{(n)} | \mathcal{H}_0) \pi(\mathcal{H}_0) + p(y^{(n)} | \mathcal{H}_1) \pi(\mathcal{H}_1)} \div \frac{ p(y^{(n)} | \mathcal{H}_1) \pi(\mathcal{H}_1) }{ p(y^{(n)}  \mathcal{H}_0) \pi(\mathcal{H}_0) + p(y^{(n)} | \mathcal{H}_1) \pi(\mathcal{H}_1)}\\
\\
& = \frac{ p(y^{(n)} | \mathcal{H}_0) \pi(\mathcal{H}_0) }{ p(y^{(n)} | \mathcal{H}_0) \pi(\mathcal{H}_0) + p(y^{(n)} | \mathcal{H}_1) \pi(\mathcal{H}_1)} \times \frac{ p(y^{(n)} | \mathcal{H}_0) \pi(\mathcal{H}_0) + p(y^{(n)} | \mathcal{H}_1) \pi(\mathcal{H}_1)}{ p(y^{(n)} | \mathcal{H}_1) \pi(\mathcal{H}_1) }\\
\\
\therefore \underbrace{\frac{\pi(\mathcal{H}_0 \mid Y)}{\pi(\mathcal{H}_1 \mid Y)}}_{\text{posterior odds}} & = \underbrace{\frac{ \pi(\mathcal{H}_0) }{ \pi(\mathcal{H}_1) }}_{\text{prior odds}} \times \underbrace{\frac{ p(y^{(n)} \mid \mathcal{H}_0) }{ p(y^{(n)} \mid \mathcal{H}_1) }}_{\text{Bayes factor } \mathcal{BF}_{01}} \\
\end{split}
$$
]
]

--

- Therefore, the Bayes factor can be thought of as the factor by which our prior odds change (towards the posterior odds) in the light of the data.




---
##  Likelihoods & Evidence

Maximized Likelihood
```{r lik, fig.height=4.5,echo=FALSE}
n = 10; y = 1.96/sqrt(n)
th <- seq(y - 3/sqrt(n), y + 3/sqrt(n), length=1000)
plot(th,dnorm(th,y,sqrt(1/n)),
      type="l", lwd=2,
      col=1,xlab=expression(theta),
      ylab=expression(l(theta)))
#m = integrate(f = function(x) {dbinom(y,n,x)}, low=0.00001, upper=.99999)
#abline(h=m$value, lty=4, lwd=2, col="red")
segments(0, 0, 0, dnorm(0, y, sqrt(1/n)), lty=2, lwd=2, col=2)
abline(h=dnorm(y,y, sqrt(1/n)), lty=2, lwd=2, col="blue")

pval = pnorm(0,  abs(y), sqrt(1/n))*2
```

p-value = `r round(pval, 4)`
---
##  Marginal Likelihoods & Evidence

Maximized Likelihood
```{r marglik, fig.height=4.5, echo=F}
v0 = 1; tau0 = 1/v0
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
theta.n = n*y/(n + tau0)
tau.n = n + tau0
plot(th,dnorm(th,y,sqrt(1/n)),
      type="l", lwd=2, ylim=c(0, dnorm(theta.n, theta.n, sqrt(1/tau.n))),
      col="blue",xlab=expression(theta),
      ylab=expression(pi(theta)))
lines(th, dnorm(th, 0, sqrt(v0)), col="red", lwd=2)
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
segments(0, 0, 0, marg.H1, lty=2, lwd=2, col="purple")
abline(h =marg.H1, lty=2, lwd=3, col="purple")
#m = integrate(f = function(x) {dbinom(y,n,x)}, low=0.00001, upper=.99999)
#abline(h=m$value, lty=4, lwd=2, col="red")
lines(th, dnorm(th, theta.n, sqrt(1/tau.n)), lty=3,  lwd=3, col="purple")
marg.H0 = dnorm(0, y, sqrt(1/n))
segments(0, 0, 0, marg.H0, lty=2, lwd=2, col="blue")
abline(h=dnorm(y,y, sqrt(1/n)), lty=3, lwd=2, col="blue")
bf10 = marg.H1/marg.H0
bf01 = 1/bf10
post.prob.H1 = 1/(1 + bf01)

#cand = dnorm(0,0, sqrt(v0))/dnorm(0, theta.n, sqrt(1/tau.n))
```

  $\cal{BF}_{10}$ = `r round(bf10, 2)`
  
  Posterior Probability of $\cal{H}_0$ = `r round(1 - post.prob.H1, 4)`
  
---
## Candidate's Formula (Besag 1989)


Alternative expression for Bayes Factor

$$\frac{p(y^{(n)} \mid \cal{H}_1)}
       {p(y^{(n)} \mid \cal{H}_0)} =
  \frac{\pi_\theta(0 \mid \cal{H}_1)} 
       {\pi_\theta(0 \mid  y^{(n)}, \cal{H}_1)}$$

--

- ratio of the prior to posterior densities  for $\theta$ evaluated at zero



--

- Savage-Dickey Ratio

---
## Prior

Plots were based on a $\theta \mid \cal{H}_1 \sim \textsf{N}(0, 1)$ 

--

- centered at value for $\theta$ under $\cal{H}_0$  (goes back to Jeffreys)

--

- "unit information prior"  equivalent to a prior sample size is 1


--

- What happens if $n \to \infty$?

--

- What happens of $\tau_0 \to 0$ ?

---
## Precision

```{r marglik2, fig.height=4.5, echo=F, out.width="50%"}
v0 = 1000; tau0 = 1/v0
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
theta.n = n*y/(n + tau0)
tau.n = n + tau0
plot(th,dnorm(th,y,sqrt(1/n)),
      type="l", lwd=2, ylim=c(0, dnorm(theta.n, theta.n, sqrt(1/tau.n))),
      col="blue",xlab=expression(theta),
      ylab=expression(pi(theta)))
lines(th, dnorm(th, 0, sqrt(v0)), col="red", lwd=2)
marg.H1 = dnorm(0, y, sqrt(v0 + 1/n))
segments(0, 0, 0, marg.H1, lty=2, lwd=2, col="purple")
abline(h =marg.H1, lty=2, lwd=3, col="purple")
#m = integrate(f = function(x) {dbinom(y,n,x)}, low=0.00001, upper=.99999)
#abline(h=m$value, lty=4, lwd=2, col="red")
lines(th, dnorm(th, theta.n, sqrt(1/tau.n)), lty=3,  lwd=3, col="purple")
marg.H0 = dnorm(0, y, sqrt(1/n))
segments(0, 0, 0, marg.H0, lty=2, lwd=2, col="blue")
abline(h=dnorm(y,y, sqrt(1/n)), lty=3, lwd=2, col="blue")
bf10 = marg.H1/marg.H0
bf01 = 1/bf10
post.prob.H1 = 1/(1 + bf01)
```

- $\tau_0 = 1/`r v0`$
--

- Posterior Probability of $\cal{H}_0$ = `r round(1 - post.prob.H1, 4)`

--

- As $\tau_0 \to 0$ the posterior probability of $\cal{H}_1$ goes to 0!

--

**Bartlett's Paradox** - the paradox is that a seemingly non-informative prior for $\theta$ is very informative about $\cal{H}$!