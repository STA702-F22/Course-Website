---
title: "Lecture 12: Normal Means & Multiple Testing"
author: "Merlise Clyde"
date: "October 18"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
require(lattice)
require(MCMCpack)
require(hdrcde)
require(coda)
#require(rethinking)
set.seed(42)
```



## Normal Means Model
Suppose we have normal data with 
$$Y_i \mid \mu_i, \sigma^2 \overset{iid}{\sim} \textsf(\mu_i, \sigma^2)$$
--


- Means Model $\mu_i \overset{iid}{\sim} g$,  "random effects" distribution

--

**Multiple Testing**

- $H_{0i}: \mu_i = 0$ versus $H_{1i}: \mu_i \neq 0$ 

--

- $n$ hypotheses that may potentially be closely related,  e.g. $H_{01}$ no difference in expression  gene $i$ between cases and controls, for $n$ genes

---
## Strategy Ia

- p-value, $p_i$, or testing $H_{0i}$ versus $H_{1i}$ for each $i$

--
- $p_i < \alpha$ implies reject $H_{0i}$ in favor of $H_{1i}$,   e.g $\alpha = 0.05$

--

Limitations?

--

- overall lots of type I errors potentially  in testing over and over again 

--

- $\alpha$ is the probabibility of making a type I error in an individual test, but not the probability of the family-wise type 1 error, e.g the probability of making at least one type 1 error in the $n$ tests)

---
## Power

-  very low power (high type II error rate)  because we have a single observation per hypothesis


```{r low_power, echo=FALSE}
par(mfrow=c(1,2))
y = seq(-3,3, length=1000)
yobs = .57
plot(y, dnorm(y, 0, 1), type="l", ylab="density", main=expression(H[0])) 
abline(v=yobs)
plot(y, dnorm(y, 1, 1), type="l", ylab="density", main=expression(H[1])) 
abline(v=yobs)
```

--

- low power unless we have good separation between the two distributions (large difference relative to noise)

--

- low power may actually lead to very few type I errors even in multiple testing  but often still lots of type I and type II errors


---
## Strategy Ib

Adjust the level of each test to reflect how many tests you are conducting

--

- Probability of at least one Type I error  if tests are independent

$$ 1 - \Pr(\text{ 0 Type I errors in } n \text{ tests}) = 1 - (1 - \alpha)^{n}$$

```{r family_wise, echo=FALSE}
n = seq(1, 500, by = 5)
plot(n, 1 - .95^n, type="l", ylab="P(at least one Type 1 error)")
lines(n, 1 - (1 - .05/n)^n, lty=2, col=2)

```

- to control the increase in Type I errors with $n$ we may need to decrease the $\alpha$ threshold with $n$

---
## Classical Strategy

-   control the family-wise error rate.  Assuming independence across tests (reality ?) replace $\alpha$ with $\alpha/n$ 

--

**Bonferroni correction**:  keeps overall family wise error at $\alpha$

--

- if we have 10,000 tests $\alpha_{\textsf{Bon}} = 0.05/10000$ very small

--

- in the extremely low power setting, probably very few tests exceed the new threshhold   (conservative)



---
##  False Discovery Rate  (FDR)

- FDR threshhold $\alpha_{\textsf{FDR}}$

--

- if $p_i < \alpha_{\textsf{FDR}}$, call this is a "discovery" 

--
 
- collect all of our discoveries, say 100 out of 10,000 genes

--
 
-  we want that the proportion of discoveries that are false (i.e $H_{0}$ was actually true) to be small

--
 
- control the proportion of false discoveries at level $\alpha$ instead of individual p-values

--

- Benjamini & Hochberg (BH) (1995 JRSS-B)  propose a simple choice for $\alpha_{\textsf{FDR}}$ based on $n$ and assuming $n$ independent tests

--

- Issue: we will still have lower power in this low data scenario!  

--

- Borrow strength!


---
##  Strategy II: Hierarchical Model

$$\begin{align*} Y_i \mid \mu_i, \sigma^2  & \overset{iid}{\sim} \textsf(\mu_i, \sigma^2)\\
\mu_i & \overset{iid}{\sim} g
\end{align*}$$

--

- naive approach: choose $g$ as $N(\mu, \sigma^2_{\mu})$ & estimate $\mu$ and $\sigma^2_\mu$  (Empirical Bayes) assuming $\sigma^2 = 1$ so that
$\hat{\mu} = \bar{y}$ and $s^2_y = 1 + \hat{\sigma}^2_\mu$, so $\hat{\sigma}^2_\mu = \max(0, 1 - s^2_y)$

--

```{r EB, echo=FALSE, fig.height=5, fig.width=5, out.width="50%"}
n = 1000
H0 = rbinom(n, 1, .5)
y = H0*rnorm(n) + (1 - H0)*rnorm(n, 1,  1.5)
mu = mean(y)
s.mu = max(0, var(y) - 1)

mean = s.mu*y/( 1 + s.mu) + mu/(1 +  s.mu)
sd = sqrt(s.mu/(1 + s.mu))

n = 100
y = y[1:n]
H0 = H0[1:n]
mean = s.mu*y/( 1 + s.mu) + mu/(1 +  s.mu)
sd = sqrt(s.mu/(1 + s.mu))

plot(y, 1:n,type="n",  xlim=c(min(y) - 1.96, max(y) + 1.96), ylab="n")
title("95% CI")
segments(y - 1.96, 1:n, y + 1.96, 1:n)
abline(v=0, lwd=3)
points(y, 1:n, col=(2 - H0), pch=16)
points(mean, 1:n, col=(2 - H0), pch=18, cex=2)

segments(mean - 1.96*sd, 1:n, mean + 1.96*sd, 1:n, col=2)
```

---
## Informal approach to testing

- Conclude in favor of $H_{1i}$ if $0 \notin (\mu_{Li}, \mu_{Ui})$

--

- otherwise fail to reject

--

**Question**:  Do we expect this approach to have a huge Type I error rate exploding with $n$ (# tests)?   Why or why not?

- shrinkage and borrowing of information leads to narrower CI

--

- information from the other $y_i$s enters into the posterior for $\mu_i$ through the estimates of $\mu$ and $\sigma^2_\mu$

--

$$\mu_i \mid y_1, \ldots, y_n \sim N\left(\frac{y_i +  \hat{\mu}/\hat{\sigma}^2_\mu}{ 1 + 1/\hat{\sigma}^2_\mu}, 
\frac{1}{  1 + 1/\hat{\sigma}^2_\mu} \right)$$

--

-  when $\sigma^2_\mu$ is small credible intervals are much narrower than with MLE


---
## Hypothetical Setting

- first  $i = 1, 2, 3$ "signals"  ( $H_{1i}$ is true)

--

-  add $n - 3$  nulls   ( $H_{0i}$ is true)

--

Does throwing in more nulls lead to  more Type I errors?


- what happens to $\hat{\mu}$ and $\hat{\sigma}^2_{\mu}$?

- what happens to the credible intervals?

---
## Informal Approach B

-  an issue with the $N(\mu, \sigma^2_\mu)$ for $g$ in the hypothetical setting is that it can capture only noise and not the signals.  (signals are outliers under normal model)

--

- choose a more flexible $g$ to capture both noise and signal!

--

```{r heavy, fig.height=5, fig.width=5, out.width="50%", echo=FALSE}
mu = seq(-4, 4, length=1000)

d = 0.50*0.5* dgamma(abs(mu), 1, 3) +  .50*dt(mu, df=1)
plot(mu, d, type="l")
lines(mu, dnorm(mu))

```



---
## Local-Global Scale Mixtures of Normals

Local scale

$$\begin{align*} \mu_i \mid \lambda_i, \tau & \sim N(0, \lambda_i \tau) \\
\lambda_i & \sim f \qquad \text{   local-scale }\\
\tau & \sim h  \qquad \text{   global-scale}
\end{align*}$$



--

-  density that  is concentration around zero to shrink  noise to zero

--

-  heavy tails avoid over-shrinkage of signals  (want heavier than normal)


--

- Includes:

  - horseshoe
  - generalized double pareto
  - Dirichlet Laplace
  
---
## Note

- a single Gaussian or Double Exponential prior (Bayes Lasso)  have exponential tails same as likelihood (in the normal means problem)

--

- single parameter controls tail behaviour and concentration at zero

--

-  will overshrink the signal if there are many noise cases

--

-  Good shrinkage prior allows separate control of the concentration around zero and tails

--

- tails need to exhibit bounded influence

--

- continous versions/relaxations of a spike and slab prior

$$\mu_i \sim \pi_0 \delta_0 + (1 - \pi) g$$

--

- allows formal Bayes multiple testing $H_{0i}: \mu = 0$

--

- $\pi_0 = \Pr(H_{0i} \text{ is true})$ another unknown to learn from the data; provides automatic adjustment for multiple testing error!
